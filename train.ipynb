{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which pip\n",
    "!which python\n",
    "!pip install numpy torch scikit-learn tqdm\n",
    "!pip install pandas matplotlib torchvision IPython opencv-python opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch.distributed as dist\n",
    "import pandas as pd\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL.Image import Image\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageDraw\n",
    "import engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/workspace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    batch_size = 16\n",
    "    num_epochs = 30\n",
    "    learning_rate = 1e-3\n",
    "    num_classes = 10\n",
    "    input_size = (1600, 1600)\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_file(label_path: str) -> pd.DataFrame:\n",
    "    with open(label_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    attributes = pd.Series(data[\"attributes\"])\n",
    "    boxes = pd.DataFrame(list(map(lambda d: d[\"box2d\"], data[\"labels\"])))\n",
    "    categories = pd.Series(list(map(lambda d: d[\"category\"], data[\"labels\"])))\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        data={\n",
    "            \"category\": categories,\n",
    "            \"x1\": boxes.x1,\n",
    "            \"x2\": boxes.x2,\n",
    "            \"y1\": boxes.y1,\n",
    "            \"y2\": boxes.y2,    \n",
    "        },\n",
    "    )\n",
    "    for (key, val) in attributes.items():\n",
    "        df[key] = val\n",
    "    df[\"image_id\"] = int((label_path.split(\"/\")[-1].split(\".\")[0]).replace(\"_\", \"\").replace(\"train\", \"\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def get_padding(image):    \n",
    "    w, h = image.size\n",
    "    max_wh = np.max([w, h])\n",
    "    h_padding = (max_wh - w) / 2\n",
    "    v_padding = (max_wh - h) / 2    \n",
    "    r_pad = h_padding if h_padding % 1 == 0 else h_padding-0.5\n",
    "    b_pad = v_padding if v_padding % 1 == 0 else v_padding-0.5\n",
    "    r_pad *= 2\n",
    "    b_pad *= 2\n",
    "    padding = (0, 0, int(r_pad), int(b_pad))\n",
    "    return padding\n",
    "\n",
    "class NewPad(object):\n",
    "    def __init__(self, fill=0, padding_mode='constant'):\n",
    "        assert padding_mode in ['constant', 'edge', 'reflect', 'symmetric']\n",
    "\n",
    "        self.fill = fill\n",
    "        self.padding_mode = padding_mode\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be padded.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image: Padded image.\n",
    "        \"\"\"\n",
    "        return F.pad(img, get_padding(img), self.fill, self.padding_mode)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(padding={0}, fill={1}, padding_mode={2})'.\\\n",
    "            format(self.fill, self.padding_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL.Image import open as pil_open\n",
    "\n",
    "def label_str_to_num(label: str) -> int:\n",
    "    return int(label[0])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data: List[str], labels: List[str] = None, transform: torchvision.transforms.Compose = None, has_label: bool = False, resizing: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.has_label = has_label\n",
    "        self.resizing = resizing\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        image_path = self.data[index]\n",
    "        img = pil_open(image_path).convert(\"RGB\")\n",
    "        # width = img.size[0]\n",
    "        # height = img.size[1]\n",
    "        # print(f\"width: {width}, height: {height}\")\n",
    "        \n",
    "        if not self.has_label:\n",
    "            img = self.transform(img)\n",
    "            return img, {}\n",
    "\n",
    "        label_info = parse_json_file(self.labels[index])\n",
    "        image_id = label_info[\"image_id\"].values[0]\n",
    "        label = torch.tensor(np.array(list(map(label_str_to_num, label_info.category.values))))        \n",
    "        ratio = config.input_size[0]/max(img.size) if self.resizing else 1\n",
    "        boxes = [\n",
    "            # label_info.x1 * config.input_size[1] / width,\n",
    "            # label_info.y1 * config.input_size[0] / height, \n",
    "            # label_info.x2 * config.input_size[1] / width + 1, \n",
    "            # label_info.y2 * config.input_size[0] / height + 1,\n",
    "            label_info.x1 * ratio,\n",
    "            label_info.y1 * ratio, \n",
    "            label_info.x1 * ratio + (label_info.x2 - label_info.x1) * ratio + 1, \n",
    "            label_info.y1 * ratio + (label_info.y2 - label_info.y1) * ratio + 1, \n",
    "        ]\n",
    "        for i in range(len(boxes)):\n",
    "            boxes[i] = torch.tensor((boxes[i]).values)\n",
    "        boxes = torch.stack(boxes, dim=1)\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        N = label.size()[0]\n",
    "        iscrowd = torch.zeros((N, ), dtype=torch.int64)\n",
    "        targets = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": label,\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": iscrowd,\n",
    "            \"image_id\": torch.tensor([image_id])            \n",
    "        }\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            assert False\n",
    "        \n",
    "        return img, targets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)        \n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "        num_classes = config.num_classes\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, img, targets = [{}], train = False):\n",
    "        if train:            \n",
    "            return self.model(img, targets)\n",
    "        else:\n",
    "            return self.model(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "annotation_path = pathlib.Path(\"train\") / \"annotations\" / \"*.json\"\n",
    "labels = list(glob(annotation_path.absolute().as_posix()))\n",
    "new_labels = []\n",
    "\n",
    "images_path = pathlib.Path(\"train\") / \"images\" / \"*.jpg\"\n",
    "images = list(glob(images_path.absolute().as_posix()))\n",
    "# new_images = []\n",
    "# for i in range(len(images)):\n",
    "#     # 1600 x 1200ではない解像度を除外\n",
    "#     img_size = pil_open(images[i]).convert(\"RGB\").size\n",
    "#     if img_size == (1600, 1200):\n",
    "#         new_images.append(images[i])\n",
    "#         new_labels.append(labels[i])\n",
    "# images = new_images\n",
    "# labels = new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, train_labels, val_labels = train_test_split(images, labels)\n",
    "\n",
    "del images\n",
    "del labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = CustomModel().to(device)\n",
    "model = nn.DataParallel(model).to(device)\n",
    "model.load_state_dict(torch.load(\"faster_rcnn_weight-1680925955.4584112.pth\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(\n",
    "    labels=train_labels,\n",
    "    data=train_images,\n",
    "    has_label=True,\n",
    "    transform=torchvision.transforms.Compose(transforms=[   \n",
    "        NewPad(),\n",
    "        transforms.Resize(config.input_size),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    ")\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, collate_fn=custom_collate)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Val preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CustomDataset(\n",
    "    labels=val_labels, \n",
    "    data=val_images, \n",
    "    has_label=True,\n",
    "    resizing=True,\n",
    "    transform=torchvision.transforms.Compose(transforms=[\n",
    "        NewPad(),\n",
    "        transforms.Resize(config.input_size),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    ")\n",
    "val_dataloader = DataLoader(dataset=dataset, batch_size=config.batch_size, shuffle=False, num_workers=4, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img, target):\n",
    "    img = torchvision.transforms.ToPILImage()(img)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    for bbox, label in zip(target[\"boxes\"], target[\"labels\"]):\n",
    "        label = label.item()\n",
    "        origin = (bbox[0], bbox[1])\n",
    "        rect = (origin, (bbox[2], bbox[3]))\n",
    "        draw.text(xy=origin, text=str(label))\n",
    "        draw.rectangle(xy=rect, outline=(255, 0, 0))\n",
    "        \n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target = dataset[1]\n",
    "show(img, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #結果の表示\n",
    "\n",
    "# def show(val_dataloader):\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     from PIL import ImageDraw, ImageFont\n",
    "#     from PIL import Image\n",
    "    \n",
    "#     #GPUのキャッシュクリア\n",
    "#     import torch\n",
    "#     torch.cuda.empty_cache()\n",
    "   \n",
    "#     device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
    "#     #device = torch.device('cpu')    \n",
    "#     model.to(device)\n",
    "#     model.eval()#推論モードへ\n",
    "\n",
    "#     images, targets = next(iter(val_dataloader))\n",
    "\n",
    "#     images = list(img.to(device) for img in images)\n",
    "    \n",
    "#     #推論時は予測を返す\n",
    "#     '''\n",
    "#      - boxes (FloatTensor[N, 4]): the predicted boxes in [x1, y1, x2, y2] format, with values of x\n",
    "#           between 0 and W and values of y between 0 and H\n",
    "#         - labels (Int64Tensor[N]): the predicted labels for each image\n",
    "#         - scores (Tensor[N]): the scores or each prediction\n",
    "#     '''\n",
    "#     outputs = model(images)\n",
    "\n",
    "#     for i, image in enumerate(images):\n",
    "\n",
    "#         image = image.permute(1, 2, 0).cpu().numpy()\n",
    "#         image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "\n",
    "#         boxes = outputs[i][\"boxes\"].data.cpu().numpy()\n",
    "#         scores = outputs[i][\"scores\"].data.cpu().numpy()\n",
    "#         labels = outputs[i][\"labels\"].data.cpu().numpy()\n",
    "\n",
    "#         category={0: '0_background',1:'1_overall', 2:'2_handwritten',3: '3_typography',4: '4_illustration', 5:'5_stamp', 6:'6_headline', 7:'7_caption',8: '8_textline', 9:'9_table'}\n",
    "\n",
    "\n",
    "\n",
    "#         boxes = boxes[scores >= 0.5].astype(np.int32)\n",
    "#         scores = scores[scores >= 0.5]\n",
    "\n",
    "#         for i, box in enumerate(boxes):\n",
    "#             draw = ImageDraw.Draw(image)\n",
    "#             label = category[labels[i]]\n",
    "#             draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=\"red\", width=3)\n",
    "\n",
    "#             # ラベルの表示\n",
    "\n",
    "#             from PIL import Image, ImageDraw, ImageFont \n",
    "#             #fnt = ImageFont.truetype('/content/mplus-1c-black.ttf', 20)\n",
    "#             fnt = ImageFont.load_default()\n",
    "#             text_w, text_h = fnt.getsize(label)\n",
    "#             draw.rectangle([box[0], box[1], box[0]+text_w, box[1]+text_h], fill=\"red\")\n",
    "#             draw.text((box[0], box[1]), label, font=fnt, fill='white')\n",
    "            \n",
    "#         #画像を保存したい時用\n",
    "#         #image.save(f\"resample_test{str(i)}.png\")\n",
    "\n",
    "#         fig, ax = plt.subplots(1, 1)\n",
    "#         ax.imshow(np.array(image))\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# show(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def train(epoch: int):\n",
    "    bar = tqdm(dataloader)\n",
    "    # train  \n",
    "    model.train()\n",
    "    for imgs, d_targets in bar: \n",
    "        imgs = imgs.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in d_targets]\n",
    "        loss_dict = model(imgs, targets, True)\n",
    "        loss = sum(loss_dict.values())\n",
    "        bar.set_description(f\"train loss: {loss}\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    # val\n",
    "    # 検証フェーズ\n",
    "    model.eval()    \n",
    "    with torch.no_grad():\n",
    "        eval = engine.evaluate(model, val_dataloader, device)        \n",
    "        imgs = eval.eval_imgs\n",
    "        for iou_type in eval.iou_types:\n",
    "            print(\"Result:\")\n",
    "            print(\" iou type:\", iou_type)\n",
    "            print(\" \", imgs[iou_type])\n",
    "\n",
    "main_bar = tqdm(range(1, config.num_epochs+1))\n",
    "for epoch in main_bar:\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "torch.save(model.state_dict(), f'faster_rcnn_weight-{time.time()}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
