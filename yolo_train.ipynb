{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/venv/bin/pip\n",
      "/workspace/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which pip\n",
    "!which python\n",
    "# !pip install numpy torch scikit-learn tqdm\n",
    "# !pip install pandas ultralytics matplotlib torchvision IPython opencv-python opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch.distributed as dist\n",
    "import pandas as pd\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL.Image import Image\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageDraw\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/workspace\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.58 ðŸš€ Python-3.8.13 torch-2.0.0+cu117 CUDA:0 (NVIDIA RTX A4000, 16117MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8x.pt, data=book.yaml, epochs=5, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=/workspace/runs/detect/train\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2320  ultralytics.nn.modules.Conv                  [3, 80, 3, 2]                 \n",
      "  1                  -1  1    115520  ultralytics.nn.modules.Conv                  [80, 160, 3, 2]               \n",
      "  2                  -1  3    436800  ultralytics.nn.modules.C2f                   [160, 160, 3, True]           \n",
      "  3                  -1  1    461440  ultralytics.nn.modules.Conv                  [160, 320, 3, 2]              \n",
      "  4                  -1  6   3281920  ultralytics.nn.modules.C2f                   [320, 320, 6, True]           \n",
      "  5                  -1  1   1844480  ultralytics.nn.modules.Conv                  [320, 640, 3, 2]              \n",
      "  6                  -1  6  13117440  ultralytics.nn.modules.C2f                   [640, 640, 6, True]           \n",
      "  7                  -1  1   3687680  ultralytics.nn.modules.Conv                  [640, 640, 3, 2]              \n",
      "  8                  -1  3   6969600  ultralytics.nn.modules.C2f                   [640, 640, 3, True]           \n",
      "  9                  -1  1   1025920  ultralytics.nn.modules.SPPF                  [640, 640, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  3   7379200  ultralytics.nn.modules.C2f                   [1280, 640, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  3   1948800  ultralytics.nn.modules.C2f                   [960, 320, 3]                 \n",
      " 16                  -1  1    922240  ultralytics.nn.modules.Conv                  [320, 320, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  3   7174400  ultralytics.nn.modules.C2f                   [960, 640, 3]                 \n",
      " 19                  -1  1   3687680  ultralytics.nn.modules.Conv                  [640, 640, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  3   7379200  ultralytics.nn.modules.C2f                   [1280, 640, 3]                \n",
      " 22        [15, 18, 21]  1   8727598  ultralytics.nn.modules.Detect                [10, [320, 640, 640]]         \n",
      "Model summary: 365 layers, 68162238 parameters, 68162222 gradients, 258.2 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /workspace/train/labels.cache... 1832 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1832/1832 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /workspace/val/labels.cache... 457 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 457/457 [00:00<?, ?it/s]\n",
      "Plotting labels to /workspace/runs/detect/train/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 7 dataloader workers\n",
      "Logging results to \u001b[1m/workspace/runs/detect/train\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/5      13.8G      1.569      1.781      1.294        548        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 115/115 [01:17<00:00,  1.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.50it/s]\n",
      "                   all        457      14021      0.511      0.461      0.498      0.311\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/5      14.8G       1.24     0.9186      1.061        396        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 115/115 [01:17<00:00,  1.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:09<00:00,  1.54it/s]\n",
      "                   all        457      14021      0.621      0.577      0.587      0.375\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        3/5      14.8G      1.225     0.8598      1.041        398        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 115/115 [01:16<00:00,  1.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:09<00:00,  1.55it/s]\n",
      "                   all        457      14021       0.66      0.578      0.538       0.38\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        4/5      14.8G      1.136     0.7459      1.004        279        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 115/115 [01:17<00:00,  1.49it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:09<00:00,  1.56it/s]\n",
      "                   all        457      14021      0.672      0.687       0.73      0.523\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        5/5      14.8G       1.08     0.6877     0.9756        317        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 115/115 [01:17<00:00,  1.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:18<00:00,  1.20s/it]\n",
      "                   all        457      14021      0.742      0.672      0.724      0.492\n",
      "\n",
      "5 epochs completed in 0.130 hours.\n",
      "Optimizer stripped from /workspace/runs/detect/train/weights/last.pt, 136.7MB\n",
      "Optimizer stripped from /workspace/runs/detect/train/weights/best.pt, 136.7MB\n",
      "\n",
      "Validating /workspace/runs/detect/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.58 ðŸš€ Python-3.8.13 torch-2.0.0+cu117 CUDA:0 (NVIDIA RTX A4000, 16117MiB)\n",
      "Model summary (fused): 268 layers, 68133198 parameters, 0 gradients, 257.4 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:17<00:00,  1.15s/it]\n",
      "                   all        457      14021      0.688      0.676       0.73      0.523\n",
      "             1_overall        457        457      0.984      0.996      0.994      0.936\n",
      "         2_handwritten        457       2036      0.768      0.781      0.742      0.368\n",
      "          3_typography        457       1917      0.869      0.587       0.72      0.466\n",
      "        4_illustration        457        352      0.763      0.685      0.717      0.534\n",
      "               5_stamp        457         91      0.744      0.747      0.796      0.583\n",
      "            6_headline        457        375      0.337      0.316      0.266      0.172\n",
      "             7_caption        457        132          1      0.159      0.586      0.261\n",
      "            8_textline        457       8659      0.659      0.809      0.755       0.39\n",
      "               9_table        457          2     0.0703          1      0.995      0.995\n",
      "Speed: 0.4ms preprocess, 9.2ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1m/workspace/runs/detect/train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load a model\n",
    "model = YOLO(\"yolov8x.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "model.train(data=\"book.yaml\", epochs=5)  # train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.58 ðŸš€ Python-3.8.13 torch-2.0.0+cu117 CUDA:0 (NVIDIA RTX A4000, 16117MiB)\n",
      "Model summary (fused): 268 layers, 68133198 parameters, 0 gradients, 257.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /workspace/val/labels.cache... 457 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 457/457 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   7%|â–‹         | 2/29 [00:04<01:02,  2.32s/it]WARNING âš ï¸ NMS time limit 1.300s exceeded\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:18<00:00,  1.60it/s]\n",
      "                   all        457      14021      0.708      0.637       0.69      0.503\n",
      "             1_overall        457        457      0.987      0.969       0.98      0.925\n",
      "         2_handwritten        457       2036      0.789      0.768      0.742      0.372\n",
      "          3_typography        457       1917      0.892       0.58      0.721      0.468\n",
      "        4_illustration        457        352       0.78      0.605      0.634      0.486\n",
      "               5_stamp        457         91      0.772      0.747      0.797      0.584\n",
      "            6_headline        457        375      0.376       0.28      0.255      0.167\n",
      "             7_caption        457        132          1     0.0349      0.365      0.162\n",
      "            8_textline        457       8659      0.695      0.749      0.719      0.372\n",
      "               9_table        457          2     0.0806          1      0.995      0.995\n",
      "Speed: 0.4ms preprocess, 15.4ms inference, 0.0ms loss, 7.3ms postprocess per image\n",
      "Results saved to \u001b[1m/workspace/runs/detect/val\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.yolo.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
       "box: ultralytics.yolo.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.yolo.utils.metrics.ConfusionMatrix object at 0x7f4a4d8afe20>\n",
       "fitness: 0.5220575954738053\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.50342,     0.92467,     0.37152,     0.46766,     0.48635,     0.58432,     0.16695,     0.16227,       0.372,       0.995])\n",
       "names: {0: '0_background', 1: '1_overall', 2: '2_handwritten', 3: '3_typography', 4: '4_illustration', 5: '5_stamp', 6: '6_headline', 7: '7_caption', 8: '8_textline', 9: '9_table'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': 0.7079427443478828, 'metrics/recall(B)': 0.637054306578733, 'metrics/mAP50(B)': 0.6898204577566719, 'metrics/mAP50-95(B)': 0.5034172774423756, 'fitness': 0.5220575954738053}\n",
       "save_dir: PosixPath('/workspace/runs/detect/val')\n",
       "speed: {'preprocess': 0.40022184342732836, 'inference': 15.360645966143785, 'loss': 0.0009249806143261933, 'postprocess': 7.349466599312079}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
